{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691226d2",
   "metadata": {},
   "source": [
    "## **Feature:** Dataset Summaries\n",
    "\n",
    "**Names:** Dhruv\n",
    "\n",
    "### **What it does**\n",
    "Generates statistical summaries including descriptive statistics (mean, median, mode, variance, std, quartiles), distribution analysis, correlation matrices, and basic visualisations for dataset exploration.\n",
    "\n",
    "### **Helper Functions**\n",
    "- calculate_basic_stats(df, columns=None): Calculate mean, median, std, variance, skewness, kurtosis for numeric columns\n",
    "- calculate_five_number_summary(df, columns=None): Calculate min, Q1, median, Q3, max, IQR for numeric columns\n",
    "- calculate_mode_stats(df, columns=None): Calculate mode, frequency, unique values for all columns\n",
    "- generate_correlation_matrix(df, method='pearson'): Create correlation matrix and heatmap for numeric columns\n",
    "- create_distribution_plots(df, columns=None, max_plots=6): Create histograms for numeric columns\n",
    "- analyse_categorical_columns(df, columns=None, top_n=5): Analyse categorical columns with frequency tables\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API Key\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not OPEN_API_KEY:\n",
    "    print(\"OpenAI API Key not found\")\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import datetime\n",
    "from sklearn import preprocessing, impute\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Langchain imports\n",
    "from langchain.chat_models import ChatOpenAI  \n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60424d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_basic_stats(df, columns=None):\n",
    "    \"\"\"Calculate basic descriptive statistics for numeric columns like mean, median, std, variance, skewness, kurtosis for numeric columns\"\"\"\n",
    "    if columns is None:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    else:\n",
    "        numeric_cols = [col for col in columns if col in df.columns and df[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    stats_dict = {}\n",
    "    for col in numeric_cols:\n",
    "        stats_dict[col] = {\n",
    "            'count': df[col].count(),\n",
    "            'mean': df[col].mean(),\n",
    "            'median': df[col].median(),\n",
    "            'std': df[col].std(),\n",
    "            'variance': df[col].var(),\n",
    "            'min': df[col].min(),\n",
    "            'max': df[col].max(),\n",
    "            'skewness': df[col].skew(),\n",
    "            'kurtosis': df[col].kurtosis()\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(stats_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a090e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_five_number_summary(df, columns=None):\n",
    "    \"\"\"Calculate five number summary (min, Q1, median, Q3, max) for numeric columns\"\"\"\n",
    "    if columns is None:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    else:\n",
    "        numeric_cols = [col for col in columns if col in df.columns and df[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    summary_dict = {}\n",
    "    for col in numeric_cols:\n",
    "        summary_dict[col] = {\n",
    "            'min': df[col].min(),\n",
    "            'Q1': df[col].quantile(0.25),\n",
    "            'median': df[col].median(),\n",
    "            'Q3': df[col].quantile(0.75),\n",
    "            'max': df[col].max(),\n",
    "            'IQR': df[col].quantile(0.75) - df[col].quantile(0.25)\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(summary_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffed275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mode_stats(df, columns=None):\n",
    "    \"\"\"Calculate mode and frequency statistics for all columns\"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    \n",
    "    mode_stats = {}\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            mode_val = df[col].mode().iloc[0] if not df[col].mode().empty else None\n",
    "            mode_stats[col] = {\n",
    "                'mode': mode_val,\n",
    "                'mode_frequency': df[col].value_counts().iloc[0] if not df[col].value_counts().empty else 0,\n",
    "                'unique_values': df[col].nunique(),\n",
    "                'most_frequent_values': df[col].value_counts().head(3).to_dict()\n",
    "            }\n",
    "    \n",
    "    return mode_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_correlation_matrix(df, method='pearson'):\n",
    "    \"\"\"Generate correlation matrix and heatmap for numeric columns\"\"\"\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    if numeric_df.empty:\n",
    "        print(\"No numeric columns found for correlation analysis\")\n",
    "        return None\n",
    "    \n",
    "    correlation_matrix = numeric_df.corr(method=method)\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, linewidths=0.5)\n",
    "    plt.title(f'Correlation Matrix ({method.title()})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e57ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distribution_plots(df, columns=None, max_plots=6):\n",
    "    \"\"\"Create distribution plots and histograms for numeric columns\"\"\"\n",
    "    if columns is None:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    else:\n",
    "        numeric_cols = [col for col in columns if col in df.columns and df[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    numeric_cols = numeric_cols[:max_plots]  # Limit number of plots\n",
    "    \n",
    "    if len(numeric_cols) == 0:\n",
    "        print(\"No numeric columns found for distribution plots\")\n",
    "        return\n",
    "    \n",
    "    n_cols = min(3, len(numeric_cols))\n",
    "    n_rows = math.ceil(len(numeric_cols) / n_cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        if i < len(axes):\n",
    "            axes[i].hist(df[col].dropna(), bins=30, alpha=0.7, edgecolor='black')\n",
    "            axes[i].set_title(f'Distribution of {col}')\n",
    "            axes[i].set_xlabel(col)\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numeric_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35af88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_categorical_columns(df, columns=None, top_n=5):\n",
    "    \"\"\"Analyse categorical columns with frequency tables\"\"\"\n",
    "    if columns is None:\n",
    "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    else:\n",
    "        categorical_cols = [col for col in columns if col in df.columns and df[col].dtype in ['object', 'category']]\n",
    "    \n",
    "    categorical_analysis = {}\n",
    "    for col in categorical_cols:\n",
    "        value_counts = df[col].value_counts()\n",
    "        categorical_analysis[col] = {\n",
    "            'unique_count': df[col].nunique(),\n",
    "            'most_frequent': value_counts.index[0] if not value_counts.empty else None,\n",
    "            'most_frequent_count': value_counts.iloc[0] if not value_counts.empty else 0,\n",
    "            'top_values': value_counts.head(top_n).to_dict(),\n",
    "            'missing_count': df[col].isna().sum()\n",
    "        }\n",
    "    \n",
    "    return categorical_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summaries(df, user_query):\n",
    "    \"\"\"\n",
    "    Main function that gets called by the main router.\n",
    "    MUST take (user_query, df) and return df\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Create helper docs (Reimplement with functions)\n",
    "    helper_docs = \"\"\"\n",
    "    - calculate_basic_stats(df, columns=None): Calculate mean, median, std, variance, skewness, kurtosis for numeric columns\n",
    "    - calculate_five_number_summary(df, columns=None): Calculate min, Q1, median, Q3, max, IQR for numeric columns\n",
    "    - calculate_mode_stats(df, columns=None): Calculate mode, frequency, unique values for all columns\n",
    "    - generate_correlation_matrix(df, method='pearson'): Create correlation matrix and heatmap for numeric columns\n",
    "    - create_distribution_plots(df, columns=None, max_plots=6): Create histograms for numeric columns\n",
    "    - analyse_categorical_columns(df, columns=None, top_n=5): Analyse categorical columns with frequency tables\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create message chain\n",
    "    messages = []\n",
    "    messages.append(SystemMessage(content=f\"\"\"\n",
    "    You are a data cleaning agent trying to generate dataset summaries.\n",
    "    \n",
    "    Dataset info: Shape: {df.shape}, Sample: {df.head(3).to_string()}\n",
    "    \n",
    "    Helper functions available:\n",
    "    {helper_docs}\n",
    "\n",
    "    Libraries available:\n",
    "    - pd (pandas), np (numpy)\n",
    "    - math, re, datetime\n",
    "    - preprocessing, impute (from sklearn)\n",
    "    - plt (matplotlib.pyplot), sns (seaborn)\n",
    "    - stats (from scipy)\n",
    "    \n",
    "    Rules:\n",
    "    - Return only executable Python code, no explanations, no markdown blocks\n",
    "    - Use helper functions where possible\n",
    "    - Store final result in 'result_df'\n",
    "    - Always print results using print() statements\n",
    "    - For specific columns mentioned in query, pass them as lists to helper functions\n",
    "    - Use df.copy() at the start to avoid modifying original\n",
    "    - Set result_df = df at the end (even though df won't be modified for summaries)\n",
    "    - No explanations, just code\n",
    "\n",
    "    Examples:\n",
    "    - \"show basic stats for age and income\" -> calculate_basic_stats(df, columns=['age', 'income'])\n",
    "    - \"correlation matrix\" -> generate_correlation_matrix(df)\n",
    "    - \"summary of all data\" -> use multiple helper functions\n",
    "    - \"distribution plots\" -> create_distribution_plots(df)\n",
    "    \"\"\"))\n",
    "    messages.append(HumanMessage(content=f\"User request: {user_query}\"))\n",
    "    \n",
    "    # Call LLM with message chain\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "    response = llm.invoke(messages)\n",
    "    generated_code = response.content.strip()\n",
    "    \n",
    "    # Execute code\n",
    "    print(f\"Generated code for summaries:\\n{generated_code}\\n\")\n",
    "    try:\n",
    "        original_df = df.copy()\n",
    "        exec_globals = {\n",
    "            'df': df,\n",
    "            'pd': pd,\n",
    "            'np': np,\n",
    "            'math': math,\n",
    "            'plt': plt,\n",
    "            'sns': sns,\n",
    "            'stats': stats,\n",
    "            'calculate_basic_stats': calculate_basic_stats,\n",
    "            'calculate_five_number_summary': calculate_five_number_summary,\n",
    "            'calculate_mode_stats': calculate_mode_stats,\n",
    "            'generate_correlation_matrix': generate_correlation_matrix,\n",
    "            'create_distribution_plots': create_distribution_plots,\n",
    "            'analyze_categorical_columns': analyse_categorical_columns,\n",
    "            'print': print\n",
    "        }\n",
    "        exec(generated_code)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Generated Code:{generated_code}\")\n",
    "        return original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST OUT YOUR FEATURE\n",
    "\n",
    "## Import Data\n",
    "\n",
    "## Run code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-cleaning-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
