{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e78be46a",
   "metadata": {},
   "source": [
    "## **Feature:** Pattern Matching\n",
    "\n",
    "**Names:** Tanat\n",
    "\n",
    "### **What it does**\n",
    "[Brief description]\n",
    "\n",
    "### **Helper Functions**\n",
    "[List Helper Functions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2cf187d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API Key\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"OpenAI API Key not found\")\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "pd.set_option('future.no_silent_downcasting', False)\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import datetime\n",
    "from pint import UnitRegistry\n",
    "from pint.errors import UndefinedUnitError\n",
    "ureg = UnitRegistry()\n",
    "\n",
    "# Langchain imports\n",
    "from langchain.chat_models import ChatOpenAI  \n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dde2f2",
   "metadata": {},
   "source": [
    "**CLASSIFYING PATTERNS**\n",
    "-\n",
    "<hr>\n",
    "\n",
    "**FUNCTIONS**\n",
    "- classify_object_columns(df, patterns)\n",
    "- classify_column(series,patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "43cf0176",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {\n",
    "    # Multiselect patterns - \"A; B\", \"A/B\", {'X', 'Y', 'Z'}, [1, 2, 3],\n",
    "    \"multiselect\": r\"^\\s*[^,;/|&]+(?:\\s*[,;/|&]\\s*[^,;/|&]+|\\s+(?:and|AND)\\s+[^,;/|&]+)+\\s*$\",\n",
    "    \"multiselect_structured\": r\"^\\s*[\\[\\{]\\s*[^,\\]\\}]+(?:\\s*,\\s*[^,\\]\\}]+)+\\s*[\\]\\}]\\s*$\",\n",
    "\n",
    "    # Numerical values\n",
    "    \"numeric_plain\": r\"^\\s*[-+]?(?:\\d{1,3}(?:[,.\\s]\\d{3})*(?:[,.]\\d+)?|\\d+(?:[,.]\\d+)?)\\s*$\",\n",
    "    \"percentage\": r\"^\\s*[-+]?\\d+(?:[,.]\\d+)?\\s*%\\s*$\",\n",
    "    \"currency\": r\"^\\s*[$€£¥₹¢₽₦₴₪₩]?\\s*[-+]?(?:\\d{1,3}(?:[,.\\s]\\d{3})*(?:[,.]\\d+)?|\\d+(?:[,.]\\d+)?)\\s*(?:USD|EUR|AUD|GBP|INR|JPY|CAD|CHF|SEK|NOK|DKK|CNY|KRW|RUB|BRL|MXN)?\\s*$\",\n",
    "    \"range\": r\"^(?:[^\\d]+)?([-+]?\\d+(?:[.,:]?\\d+)?)\\s*(?:[-–—]\\s*|\\s+(?:to|TO|bis|à|and|AND)\\s+)\\s*([-+]?\\d+(?:[.,:]?\\d+)?)(?:\\s+([a-zA-Z ]+))?\\s*$\",\n",
    "    \"numeric_with_units\": r\"^\\s*(?:\\w+(?:\\s+\\w+)*)?\\s*[-+]?\\d+(?:[,.]\\d+)?\\s*(?:[-–—]\\s*|\\s+(?:to|TO)\\s+)\\s*[-+]?\\d+(?:[,.]\\d+)?\\s*(?:\\w+(?:\\s+\\w+)*)?\\s*$\",\n",
    "    \"scientific\": r\"^\\s*[-+]?\\d+(?:[,.]\\d+)?[eE][-+]?\\d+\\s*$\",   \n",
    "    \n",
    "    # Date-time patterns (not  tested)\n",
    "    \"date_like\": r\"^\\s*(?:\\d{1,2}[/.-]\\d{1,2}[/.-]\\d{2,4}|\\d{4}[/.-]\\d{1,2}[/.-]\\d{1,2})\\s*$\",\n",
    "    \"time_like\": r\"^\\s*(?:\\d{1,2}:\\d{2}(?::\\d{2})?(?:\\s*[APap][Mm])?)\\s*$\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fc25f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_object_columns(df, patterns):\n",
    "    col_types = {}\n",
    "    object_df = df.select_dtypes(include=['object'])\n",
    "    for col in object_df:\n",
    "        col_type = classify_column(df[col], patterns)\n",
    "        if col_type:\n",
    "            col_types[col] = col_type\n",
    "    return col_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "81ab6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_column(series, patterns=patterns):\n",
    "    \"\"\"\n",
    "    Classify an object column into a numeric-like pattern if most values match.\n",
    "    \"\"\"\n",
    "    # Remove null values and convert to string\n",
    "    vals = series.dropna().astype(str)\n",
    "    if vals.empty:\n",
    "        return None\n",
    "    \n",
    "    # Get unique values - much more efficient than sampling\n",
    "    unique_vals = vals.unique()[0:30]\n",
    "    total_unique = len(unique_vals)\n",
    "\n",
    "    if total_unique < 10:\n",
    "        min_support = 0.5\n",
    "    elif total_unique < 30:\n",
    "        min_support = 0.6\n",
    "    else:\n",
    "        min_support = 0.7\n",
    "\n",
    "    # Check each pattern\n",
    "    for pattern_name, pattern_regex in patterns.items():\n",
    "        matches = 0\n",
    "        for val in unique_vals:\n",
    "            if re.match(pattern_regex, val.strip()):\n",
    "                matches += 1\n",
    "\n",
    "        # If this pattern matches enough values, return it\n",
    "        if matches / total_unique >= min_support:\n",
    "            return pattern_name\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e678e101",
   "metadata": {},
   "source": [
    "**HANDLING NUMERIC COLUMNS**\n",
    "-\n",
    "<hr>\n",
    "\n",
    "**FUNCTIONS**\n",
    "- clean_numeric_plain(val)\n",
    "- handle_numeric(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7650271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numeric_plain(val):\n",
    "    s = str(val).strip()\n",
    "\n",
    "    # Remove spaces, symbols, currency codes, %\n",
    "    s = re.sub(r'\\s+', '', s)\n",
    "    s = re.sub(r'[$€£¥₹¢₽₦₴₪₩]', '', s)\n",
    "    s = re.sub(r'(USD|EUR|AUD|GBP|INR|JPY|CAD|CHF|SEK|NOK|DKK|CNY|KRW|RUB|BRL|MXN)', '', s, flags=re.IGNORECASE)\n",
    "    is_percent = False\n",
    "    if '%' in s:\n",
    "        s = s.replace('%', '')\n",
    "        is_percent = True\n",
    "\n",
    "    # Handle different number formats: 1.234,56 or 1,234.56\n",
    "    if re.match(r'^\\d{1,3}(\\.\\d{3})+,\\d+$', s):\n",
    "        s = s.replace('.', '').replace(',', '.')\n",
    "    elif re.match(r'^\\d{1,3}(,\\d{3})+(\\.\\d+)?$', s):\n",
    "        s = s.replace(',', '')\n",
    "\n",
    "    # Handle any decimal commas and stray commas\n",
    "    elif re.match(r'^\\d+,\\d+$', s):\n",
    "        s = s.replace(',', '.')\n",
    "    s = s.replace(',', '')\n",
    "\n",
    "    # Convert to float\n",
    "    try:\n",
    "        num = float(s)\n",
    "        if is_percent:\n",
    "            num = num / 100\n",
    "        return num\n",
    "    except:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cab7cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_numeric(series):\n",
    "    errors = {}\n",
    "    cleaned = [clean_numeric_plain(val) for val in series]\n",
    "    converted = pd.to_numeric(cleaned, errors='coerce')\n",
    "    converted = pd.Series(converted, index=series.index, name=series.name)\n",
    "    for idx, (orig, conv) in enumerate(zip(series, converted)):\n",
    "        if pd.isna(conv) and pd.notna(orig):\n",
    "            if orig not in errors:\n",
    "                errors[orig] = 0\n",
    "            errors[orig] += 1\n",
    "            converted.iloc[idx] = orig\n",
    "    return converted, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae21900",
   "metadata": {},
   "source": [
    "**HANDLING UNIT COLUMNS**\n",
    "-\n",
    "<hr>\n",
    "\n",
    "ureg: pint's unit registry\n",
    "\n",
    "**FUNCTIONS**\n",
    "- unit_exists(unit, ureg)\n",
    "- handle_unit(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4558ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to check if pint unit exists\n",
    "def unit_exists(unit, ureg):\n",
    "    try:\n",
    "        ureg.Unit(unit)\n",
    "        return True\n",
    "    except UndefinedUnitError:\n",
    "        return False\n",
    "\n",
    "def handle_units(series):\n",
    "    \"\"\"\n",
    "    Normalize all units to the first unit found in the series and return numeric values.\n",
    "    \"\"\"\n",
    "    errors = {}\n",
    "    ureg = UnitRegistry()    \n",
    "    # Find the first unit in the series\n",
    "    target_unit = None\n",
    "    for text in series.dropna():\n",
    "        match = re.search(r'(\\d+\\.?\\d*)\\s*([a-zA-Z]+)', str(text))\n",
    "        if match and unit_exists(match.group(2), ureg):\n",
    "            target_unit = match.group(2)\n",
    "            break\n",
    "        else:\n",
    "            if str(text) not in errors:\n",
    "                errors[str(text)] = 0\n",
    "            errors[str(text)] += 1\n",
    "    \n",
    "    if not target_unit:\n",
    "        return series, errors\n",
    "\n",
    "    # Helper function within to turn unit strings into numbers\n",
    "    def convert_units(text):\n",
    "        # Handle no match or null\n",
    "        if pd.isna(text):\n",
    "            return None\n",
    "        match = re.search(r'(\\d+\\.?\\d*)\\s*([a-zA-Z]+)', str(text))\n",
    "        if not match:\n",
    "            if str(text) not in errors:\n",
    "                errors[str(text)] = 0\n",
    "            errors[str(text)] += 1\n",
    "            return text\n",
    "\n",
    "        value, unit = match.groups()\n",
    "        try:\n",
    "            quantity = ureg.Quantity(float(value), unit)\n",
    "            converted = quantity.to(target_unit)\n",
    "            return converted.magnitude\n",
    "        except Exception as e:\n",
    "            if str(text) not in errors:\n",
    "                errors[str(text)] = 0\n",
    "            errors[str(text)] += 1\n",
    "            return text\n",
    "            \n",
    "    # Apply convert_unit function\n",
    "    numeric_series = series.apply(convert_units) \n",
    "    numeric_series.name = f\"{series.name or 'values'}_{target_unit}\"\n",
    "    return numeric_series, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333f3f1",
   "metadata": {},
   "source": [
    "**HANDLING RANGE COLUMNS**\n",
    "-\n",
    "<hr>\n",
    "\n",
    "**FUNCTIONS**\n",
    "- convert_range(val)\n",
    "- handle_range(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8b2fe87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_range(val):\n",
    "    \"\"\"\n",
    "    Extracts numeric ranges and units from strings, computes the midpoint.\n",
    "    Returns a string \"{midpoint} {unit}\"\n",
    "    \"\"\"\n",
    "    s = str(val).strip()\n",
    "    # Match the range pattern\n",
    "    range_regex = r\"^(?:[^\\d]+)?([-+]?\\d+(?:[.,:]?\\d+)?)\\s*(?:[-–—]\\s*|\\s+(?:to|TO|bis|à|and|AND)\\s+)\\s*([-+]?\\d+(?:[.,:]?\\d+)?)(?:\\s+([a-zA-Z ]+))?\\s*$\"\n",
    "    match = re.match(range_regex, s)\n",
    "    if not match:\n",
    "        return s\n",
    "    num1, num2, unit = match.group(1), match.group(2), match.group(3)\n",
    "    # Normalize decimals\n",
    "    num1 = num1.replace(',', '.').replace(':', '.')\n",
    "    num2 = num2.replace(',', '.').replace(':', '.')\n",
    "    try:\n",
    "        n1 = float(num1)\n",
    "        n2 = float(num2)\n",
    "        midpoint = (n1 + n2) / 2\n",
    "    except:\n",
    "        return s\n",
    "    return f\"{midpoint} {unit}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "077ef89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_range(series):\n",
    "    \"\"\"\n",
    "    Applies handle_range to a pandas Series.\n",
    "    Returns a pandas Series with \"{midpoint} {unit}\" or np.nan if parsing fails.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    errors = {}\n",
    "    valid_mask = []\n",
    "    for val in series:\n",
    "        if val is None or (isinstance(val, float)):\n",
    "            result.append(val)\n",
    "            valid_mask.append(False)\n",
    "            continue\n",
    "        cleaned = handle_range(val)\n",
    "        if isinstance(cleaned, str) and cleaned == str(val).strip():\n",
    "            if val not in errors:\n",
    "                errors[val] = 0\n",
    "            errors[val] += 1\n",
    "            result.append(val)\n",
    "            valid_mask.append(False)\n",
    "        else:\n",
    "            result.append(cleaned)\n",
    "            valid_mask.append(True)\n",
    "\n",
    "    result = pd.Series(result)\n",
    "    result.name = series.name\n",
    "\n",
    "    valid_series = result[valid_mask]\n",
    "    converted, unit_errors = handle_units(valid_series)\n",
    "    result.name = converted.name\n",
    "    errors.update(unit_errors)\n",
    "\n",
    "    # Update only valid positions in the result\n",
    "    result.loc[valid_series.index] = converted\n",
    "    return result, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4e7faf",
   "metadata": {},
   "source": [
    "**CONVERTING COLUMNS**\n",
    "-\n",
    "<hr>\n",
    "\n",
    "**FUNCTIONS**\n",
    "- classify_and_convert(df, patterns)\n",
    "- llm_fix_errors(df, all_errors)\n",
    "- object_conversion_with_llm(df, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "81a97901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_convert(df, patterns):\n",
    "    \"\"\"\n",
    "    Classifies object columns and applies appropriate conversion.\n",
    "    \"\"\"\n",
    "    col_types = classify_object_columns(df,patterns)\n",
    "    all_errors = {}\n",
    "    if not col_types:\n",
    "        print(\"No patterns detected\")\n",
    "        return df, all_errors\n",
    "\n",
    "    print(\"Classified columns, Attempting Inital conversion\")\n",
    "    for col, col_type in col_types.items():\n",
    "        if col_type in {'multiselect, multiselect_structured'}:\n",
    "            pass \n",
    "        elif col_type == 'range':\n",
    "            converted, errors = handle_range(df[col])\n",
    "        elif col_type in {'percentage', 'numeric_plain', 'currency', 'scientific'}:\n",
    "            converted, errors = handle_numeric(df[col])\n",
    "        elif col_type == 'numeric_with_units':\n",
    "            converted, errors = handle_units(df[col])\n",
    "        else:\n",
    "            continue\n",
    "        # Update df column and rename if name changes\n",
    "        df[col] = converted\n",
    "        if converted.name != col:\n",
    "            df.rename(columns={col: converted.name}, inplace=True)\n",
    "            col_key = converted.name\n",
    "        else:\n",
    "            col_key = col\n",
    "        # Update log with error values\n",
    "        if errors:\n",
    "            all_errors[col_key] = {}\n",
    "            all_errors[col_key]['errors'] = errors\n",
    "            all_errors[col_key]['type'] = col_type\n",
    "    print(\"Initial conversion attempt completed\")\n",
    "    return df, all_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "46717c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_fix_errors(df, all_errors):\n",
    "    prompt = \"\"\"\n",
    "    The following columns have already been attempted to convert to numerical columns.\n",
    "    Please suggest Python code to fix these by suggesting appropriate numerical values to replace these error values:\\n\\n\n",
    "    - Assume dataframe already exists stored in variable 'df'\n",
    "    - Import pandas as pd and numpy as np at the start\n",
    "\n",
    "    Rules:\n",
    "    - For open-ended ranges or qualitative values, use reasonable domain assumptions and try to CALCULATE MIDPOINTS\n",
    "    (e.g., max working years is 50, max age is 100, etc.) when calculating midpoints or replacements.\n",
    "    - Print reasonings for your assumptions after any assumptions have been made\n",
    "    (e.g., print('LLM: Assumed 0.25 hours for less than 30 minutes and 5 hours for over 4')\n",
    "    - If no assumptions needed don't assume just replace strings with the appropriate numerical value\n",
    "    - ASSUMING ADDS BIAS TO DATA USE EXACT VALUE WHEN IT MAKES SENSE (NO RANGES)\n",
    "    (e.g., '59.5 employees' -> 59.5, '3.5 times per week' -> 3.5)\n",
    "    - Handle units carefully make sure units allign with other units in the column\n",
    "    - Use this EXACT format to avoid pandas warnings:\n",
    "    df[col] = df[col].replace({'old_value': new_value}).infer_objects(copy=False)\n",
    "    - ONLY convert to np.nan if data doesn't make sense/allign with context and unsure on how to convert\n",
    "    \n",
    "    Example:\n",
    "    df['Age'] = df['Age'].replace({'Under 18': 17, '65+': 70}).infer_objects(copy=False)\n",
    "    print('LLM: Assumed 17 for Under 18 and 70 for 65+ in Age column')\n",
    "    \n",
    "    ONLY GENERATE VALID PYTHON CODE (no markdown blocks, no explanations)\n",
    "    \"\"\"\n",
    "    print(\"Final LLM attempt to fix errors (Might take awhile)\")\n",
    "    errors_context = \"\"\n",
    "    df_context = \"\"\n",
    "    for col, info in all_errors.items():\n",
    "        error_type = info.get('type', 'unknown')\n",
    "        error_values = list(info['errors'].keys())\n",
    "        errors_context += f\"Column: {col}\\nType: {error_type}\\nErrors: {error_values[:10]}\\n\\n\"\n",
    "\n",
    "        df_unique = df[col].dropna().astype(str).unique()\n",
    "        df_unique_len = len(df_unique)\n",
    "        df_unique_vals = \", \".join(df_unique[:20])\n",
    "        df_context += f\"Column:{col}\\nUniques:{df_unique_len}\\nValues:{df_unique_vals}\\n\\n\"\n",
    "\n",
    "    messages = []\n",
    "    messages.append(SystemMessage(prompt))\n",
    "    messages.append(HumanMessage(errors_context))\n",
    "        \n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "    response = llm.invoke(messages)\n",
    "    generated_code = response.content.strip()\n",
    "    # print(generated_code)\n",
    "    \n",
    "    try:\n",
    "        original_df = df.copy()\n",
    "        namespace = {\"df\": df, \"pd\": pd, \"np\": np}\n",
    "        exec(generated_code, namespace)\n",
    "        df = namespace[\"df\"]\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Generated Code:{generated_code}\")\n",
    "        return original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a81be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_conversion_with_llm(df, patterns):\n",
    "    \"\"\"\n",
    "    Runs the full object column conversion pipeline:\n",
    "    1. Classifies and converts object columns.\n",
    "    2. Uses LLM to fix remaining errors.\n",
    "    3. Checks which columns are still object dtype.\n",
    "    \"\"\"\n",
    "    # Step 1: Initial classification + conversion \n",
    "    df, all_errors = classify_and_convert(df, patterns)\n",
    "    # Step 2: LLM fixes any errors\n",
    "    if all_errors:\n",
    "        failed_cols=[]\n",
    "        df = llm_fix_errors(df, all_errors)\n",
    "        # Check for remaining object columns\n",
    "        for col in all_errors.keys():\n",
    "            if col in df.columns and df[col].dtype == 'object':\n",
    "                failed_cols.append(col)\n",
    "        if not failed_cols:\n",
    "            print(\"All error columns have been successfully converted from object dtype.\")\n",
    "        else:\n",
    "            print(f\"The following columns are still object dtype: {failed_cols}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaa64be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_matching(df, user_query):\n",
    "    \"\"\"\n",
    "    Main function that gets called by the main router.\n",
    "    MUST take (user_query, df) and return df\n",
    "    \"\"\"\n",
    "    # TODO: Create helper docs (Reimplement with functions)\n",
    "    helper_docs = f\"\"\"\n",
    "    VARIABLES AVAILABLE\n",
    "    - patterns: a dict containing regex patterns in order to classify columns\n",
    "    contains patterns for: {\", \".join(patterns.keys())}\n",
    "    sample patterns: {', '.join(list(patterns.values())[0:4])}\n",
    "\n",
    "    FUNCTIONS AVAILABLE:\n",
    "    - col_type = classify_column(series,patterns)\n",
    "    takes a singular column and a dict of regex patterns then classifies them \n",
    "    returns a string, column type matched from patterns dict\n",
    "\n",
    "    - col_types = classify_object_columns(df, patterns)\n",
    "    automatically takes object columns from a df and classfies them with classify_column\n",
    "        returns \n",
    "        - a dict of matched column names and its column types e.g. {{'Smoke everyday': 'percentage', 'Company Size': 'range'}}\n",
    "    MAKE SURE TO PRINT TO SEE RESULTS\n",
    "        \n",
    "    - df = object_conversion_with_llm(df, patterns):\n",
    "    runs the full pipline classify_and_convert and takes any remaining string values \n",
    "    that couldn't be converted and gets llm to process and suggest numerical replacement values to convert column \n",
    "    returns:\n",
    "        -converted df\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create message chain\n",
    "    messages = []\n",
    "    messages.append(SystemMessage(content=f\"\"\"\n",
    "    You are a data cleaning agent.\n",
    "    \n",
    "    Dataset info: Shape: {df.shape}, Sample: {df.head(3).to_string()}\n",
    "    \n",
    "    Helper functions available:\n",
    "    {helper_docs}\n",
    "\n",
    "    Libraries available:\n",
    "    - pd (pandas), np (numpy)\n",
    "    - math, re, datetime\n",
    "    - preprocessing, impute (from sklearn)\n",
    "    \n",
    "    Rules:\n",
    "    - Return only executable Python code, no explanations, no markdown blocks\n",
    "    - Use helper functions where possible\n",
    "    - Store final result in 'df'\n",
    "    - No explanations, just code\n",
    "    \"\"\"))\n",
    "    messages.append(HumanMessage(content=f\"User request: {user_query}\"))\n",
    "    \n",
    "    # Call LLM with message chain\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "    response = llm.invoke(messages)\n",
    "    generated_code = response.content.strip()\n",
    "    # Execute code\n",
    "    try:\n",
    "        original_df = df.copy()\n",
    "        exec(generated_code)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Generated Code:{generated_code}\")\n",
    "        return original_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcdc9ce",
   "metadata": {},
   "source": [
    "**TESTING**\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fcac110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"../sample_data/smoke.csv\")[0:5000]\n",
    "df = pd.read_csv(\"../sample_data/smoke.csv\")[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "63e2ea4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Smoke everyday': 'percentage', 'Smoke some days': 'percentage', 'Former smoker': 'percentage', 'Never smoked': 'percentage'}\n"
     ]
    }
   ],
   "source": [
    "# Sample Queries:\n",
    "# 1. classify columns\n",
    "test_df = pattern_matching(df, \"Detect formatting issues\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-cleaning-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
